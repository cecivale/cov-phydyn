# ------------------------------------------------------------------------------
#          ---        
#        / o o \    Project:  cov-phydyn
#        V\ Y /V    Snakefile main analysis for SARS-CoV-2 MTBD phylodynamics
#    (\   / - \     2 December 2022 modified on 26 January 2023
#     )) /    |     
#     ((/__) ||     Code by Ceci VA 
# ------------------------------------------------------------------------------

import urllib, json
import pandas as pd

configfile: "config.yaml"
localrules: all, clean

DATASET = config["run"]["dataset"]
DEMES = config["data"][DATASET].keys()
SUBSAMPLING = config['subsample']['method']
DATA_SEED = range(0, config['subsample']['replicates'])
ANALYSIS = config['run']['beast_analysis']
BEAST_SEED = range(1, config['beast'][ANALYSIS]['replicates'] + 1)


rule all: 
    input:
        trace = expand("results/{dataset}/beast/{analysis}/{subsampling}.{dseed}.comb.log", 
            dataset = DATASET, 
            analysis = ANALYSIS, 
            subsampling = SUBSAMPLING, 
            bseed = BEAST_SEED, 
            dseed = DATA_SEED),
        ml_tree = expand("results/{dataset}/mltree/{subsampling}.{dseed}.iqtree", 
            dataset = DATASET, 
            subsampling = SUBSAMPLING, 
            dseed = DATA_SEED)


rule clean:
    shell:
        '''
        rm -rf results data logs
        '''


rule load_metadata:
    message:
        """
        Load with LAPIS all high quality (nextclade qc score between 0 and 10) human sequences metadata in GISAID for {wildcards.deme} from config file.
        """
    output:
        metadata = "results/datasets/{deme}/metadata.tsv"
    log:
        "logs/load_metadata_{deme}.txt"
    conda:
        "envs/python-genetic-data.yaml"
    shell:
        """
        python3 workflow/scripts/query_metadata.py \
            --config "{config}" \
            --deme {wildcards.deme} \
            --output {output.metadata} 2>&1 | tee {log}
        """

rule subsample:
    message:
        """
        Subsample sequences from deme {wildcards.deme} sequence metadata by subsampling method {wildcards.subsampling}, seed {wildcards.dseed}.
        """
    input:
        metadata = rules.load_metadata.output.metadata,
        cases = config["files"]["cases"]
    params:
        n = lambda wildcards: config["data"][wildcards.dataset][wildcards.deme]["n"],
        include = config["files"]["include"],
        exclude = config["files"]["exclude"],
        seq_id = config['lapis']['seq_id']
    output:
        metadata = "results/{dataset}/data/{deme}/subsample_{subsampling}.{dseed}.tsv"
    log:
        "logs/subsample_{dataset}_{deme}_{subsampling}.{dseed}.txt"
    conda:
        "envs/r-genetic-data.yaml"
    shell:
        """
        Rscript workflow/scripts/subsample.R \
            --metadata_file {input.metadata} \
            --include_file {params.include} \
            --exclude_file {params.exclude} \
            --n {params.n} \
            --method {wildcards.subsampling} \
            --seq_id {params.seq_id} \
            --cases_file {input.cases} \
            --seed {wildcards.dseed} \
            --output_file {output.metadata} 2>&1 | tee {log}
        """

def _get_subsamples(wildcards):
    files = expand("results/{dataset}/data/{deme}/subsample_{{subsampling}}.{{dseed}}.tsv",
        deme = DEMES, dataset = DATASET)
    return files


rule combine_subsamples:
    message:
        """
        Combine sequences metadata from subsamples.
        """
    input:
        subsample = _get_subsamples
    output:
        combined = "results/datasets/ids_{subsampling}.{dseed}.tsv"
    log:
        "logs/combine_subsamples_{subsampling}.{dseed}.txt"
    shell:
        """
        awk '(NR == 1) || (FNR > 1)' {input.subsample} > {output.combined} 2>&1 | tee {log}
        """

rule load_sequences:
    message:
        """
        Load with LAPIS the selected sequences for each dataset and drop not full genome sequences.
        """
    input:
        ids = rules.combine_subsamples.output.combined
    output:
        aln = "results/datasets/aln_{subsampling}.{dseed}.fasta"
    log:
        "logs/load_seqs_{subsampling}.{dseed}.txt" # TODO add log, change run to shell
    conda:
        "envs/python-genetic-data.yaml"
    shell:
        """
        python3 workflow/scripts/query_sequences.py \
            --config "{config}" \
            --ids_file {input.ids} \
            --output_file {output.aln} \
            --drop_incomplete True 2>&1 | tee {log}
        """

    
rule ml_tree:
    input:
        aln = rules.load_sequences.output.aln
    output:
        tree = "results/{dataset}/mltree/{subsampling}.{dseed}.iqtree"
    params:
        tree_args = config["ml_tree"]["tree_args"],
        file_name = "results/{dataset}/mltree/{subsampling}.{dseed}"
    conda:
        "envs/iqtree2.yaml"
    shell:
        """
        iqtree2 -s {input.aln}  \
        {params.tree_args} \
        --prefix {params.file_name} 
        rm -f results/trees/*.ckp.gz 
        rm -f results/trees/*.uniqueseq.phy 
        rm -f results/trees/*.nex 
        mv {params.file_name}.log logs/ml_tree_{wildcards.dataset}_{wildcards.subsampling}.{wildcards.dseed}.log
        """


rule beast:
    """
    Running BEAST2 analysis {wildcards.analysis} BDMM-Prime, MCMC chain {wildcards.bseed}, subsample {wildcards.subsampling} {wildcards.dseed}.
    """
    input:
        aln = rules.load_sequences.output.aln,
        ids = rules.combine_subsamples.output.combined,
        xml = "resources/beast_xml/{analysis}.xml",
    output:
        trace = "results/{dataset}/beast/{analysis}/chains/{subsampling}.{dseed}.{bseed}.f.log",
        trees = "results/{dataset}/beast/{analysis}/chains/{subsampling}.{dseed}.{bseed}.f.trees"
    params:
        jar = lambda wildcards: config["beast"][wildcards.analysis].get("jar") or config["beast"].get("jar"),
        length = lambda wildcards: config["beast"][wildcards.analysis].get("length") or config["beast"].get("length"),
        logevery = lambda wildcards: round((config["beast"][wildcards.analysis].get("length") or config["beast"].get("length")) / 10000),
        action = lambda wildcards: config["beast"][wildcards.analysis].get("action") or config["beast"].get("action"),
        demes = ",".join(list(config["data"].keys())),
        mrs = lambda wildcards: max(pd.read_csv("results/datasets/ids_" + wildcards.subsampling + "." + wildcards.dseed + ".tsv", sep = '\t')['seq_name'].str.split("|", expand=True)[2]),
        #sampling_rate_op = lambda wildcards: "ScaleOperator" if wildcards.subsampling == "uniform" else "SmartScaleOperator",
        # sampling_rate_op = "ScaleOperator",
        folder_name = "results/{dataset}/beast/{analysis}/chains",
        file_name = "{subsampling}.{dseed}.{bseed}"   
    log:
        "logs/beast_{dataset}_{analysis}_{subsampling}.{dseed}.{bseed}.txt"
    benchmark:
        "benchmarks/beast_{dataset}_{analysis}_{subsampling}.{dseed}.{bseed}.benchmark.txt"
    threads:
        lambda wildcards: config["beast"][wildcards.analysis].get("threads") or config["beast"].get("threads"),
    resources:
        runtime = lambda wildcards: config["beast"][wildcards.analysis].get("time") or config["beast"].get("time"),
        mem_mb = lambda wildcards: config["beast"][wildcards.analysis].get("mem_mb") or config["beast"].get("mem_mb")
    shell:
        """
        if test -f "{params.folder_name}/{params.file_name}.state"; then
            ACTION={params.action}
            scp {output.trace} {params.folder_name}/{params.file_name}.log 
            scp {output.trees} {params.folder_name}/{params.file_name}.trees
        else
            ACTION=overwrite
        fi

        mkdir -p {params.folder_name}
        
        java -Xmx3G -jar {params.jar} \
            -D aligned_fasta={input.aln} \
            -D demes="{params.demes}" \
            -D mrs="{params.mrs}" \
            -D chain_length={params.length} \
            -D log_every={params.logevery} \
            -D file_name={params.file_name} \
            -seed {wildcards.bseed} \
            -statefile "{params.folder_name}/{params.file_name}.state" \
            -java -$ACTION {input.xml} 2>&1 | tee -a {log}
        
        [ -f {params.folder_name}/{params.file_name}.log ] && mv {params.folder_name}/{params.file_name}.log {output.trace}
        [ -f {params.folder_name}/{params.file_name}.trees ] && mv {params.folder_name}/{params.file_name}.trees {output.trees} 
        """


def _get_trace_tocombine(wildcards):
    files = expand(
        "results/{{dataset}}/beast/{{analysis}}/chains/{{subsampling}}.{{dseed}}.{bseed}.f.log",
        bseed = BEAST_SEED)
    return files

rule combine_trace:
    message: 
        """
        Combine trace files: {input.trace_files} with LogCombiner v1.8.2.
        """
    input:
        trace_files = _get_trace_tocombine    
    output:
        combined_trace = "results/{dataset}/beast/{analysis}/{subsampling}.{dseed}.comb.log"
    log:
        "logs/combine_trace_{dataset}_{analysis}_{subsampling}.{dseed}.txt"
    benchmark:
        "benchmarks/combine_trace_{dataset}_{analysis}_{subsampling}.{dseed}.benchmark.txt"
    params:
        jar = lambda wildcards: config["beast"][wildcards.analysis].get("jar") or config["beast"].get("jar"),
        burnin = lambda wildcards: config["beast"][wildcards.analysis].get("burnin") or config["beast"].get("burnin"),
        input_command = lambda wildcards, input: " -log ".join(input) 
    shell:
        """
        java -cp {params.jar} beast.app.tools.LogCombinerLauncher -log {params.input_command} -o {output.combined_trace} -b {params.burnin}  2>&1 | tee -a {log}
        """

